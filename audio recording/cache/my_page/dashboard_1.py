import streamlit as st
import pandas as pd
import time
import random
from datetime import datetime, timedelta
import os
import logging

from flask_login import user_logged_in

from core.session_manager import get_session_value, set_session_value


# G√©n√©ration de donn√©es fictives pour le dashboard
def generate_mock_data():
    """
    G√©n√®re des donn√©es fictives pour le dashboard.
    Utilise un seed bas√© sur la date pour g√©n√©rer des donn√©es coh√©rentes
    mais qui √©voluent chaque jour.
    """
    # Seed bas√© sur la date (change chaque jour)
    today = datetime.now().date()
    seed = int(today.strftime("%Y%m%d"))
    random.seed(seed)

    # Date de d√©but (30 jours en arri√®re)
    start_date = today - timedelta(days=30)

    # Cr√©ation des dates
    dates = [start_date + timedelta(days=i) for i in range(31)]

    # G√©n√©ration des m√©triques
    transcription_counts = [random.randint(5, 20) for _ in range(31)]
    youtube_extractions = [random.randint(3, 15) for _ in range(31)]
    video_extractions = [random.randint(2, 10) for _ in range(31)]
    tts_generations = [random.randint(1, 8) for _ in range(31)]

    # Ajout d'une tendance (croissance progressive)
    for i in range(1, 31):
        growth_factor = 1 + (i / 100)
        transcription_counts[i] = int(transcription_counts[i - 1] * growth_factor)
        youtube_extractions[i] = int(youtube_extractions[i - 1] * (1 + (i / 150)))
        video_extractions[i] = int(video_extractions[i - 1] * (1 + (i / 200)))
        tts_generations[i] = int(tts_generations[i - 1] * (1 + (i / 180)))

    # Cr√©ation du DataFrame
    df = pd.DataFrame({
        'date': dates,
        'transcriptions': transcription_counts,
        'youtube_extractions': youtube_extractions,
        'video_extractions': video_extractions,
        'tts_generations': tts_generations
    })

    return df


def get_user_usage_metrics(user_id):
    """
    R√©cup√®re les m√©triques d'utilisation r√©elles de l'utilisateur.
    """
    from core.database import UserActivity, get_db
    from sqlalchemy import func, and_, or_
    import pandas as pd
    import datetime

    try:
        db = next(get_db())

        # Date de d√©but (30 jours en arri√®re)
        today = datetime.datetime.now().date()
        start_date = today - datetime.timedelta(days=30)
        start_datetime = datetime.datetime.combine(start_date, datetime.time.min)

        # Requ√™te pour obtenir les activit√©s par jour et type
        activities = db.query(
            func.date(UserActivity.created_at).label('date'),
            UserActivity.activity_type,
            func.count().label('count')
        ).filter(
            and_(
                UserActivity.user_id == user_id,
                UserActivity.created_at >= start_datetime
            )
        ).group_by(
            func.date(UserActivity.created_at),
            UserActivity.activity_type
        ).all()

        # D√©bogage - afficher les activit√©s trouv√©es dans les logs
        logging.info(f"Activit√©s trouv√©es pour l'utilisateur {user_id}: {len(activities)}")
        for act in activities:
            logging.info(f"Date: {act.date}, Type: {act.activity_type}, Count: {act.count}")

        # Cr√©er un DataFrame avec toutes les dates
        all_dates = [start_date + datetime.timedelta(days=i) for i in range(31)]
        data_rows = []

        # Initialiser avec des z√©ros
        for date in all_dates:
            row = {
                'date': date,
                'transcriptions': 0,
                'youtube_extractions': 0,
                'video_extractions': 0,
                'tts_generations': 0
            }
            data_rows.append(row)

        df = pd.DataFrame(data_rows)

        # Remplir avec les donn√©es r√©elles
        for activity in activities:
            date_val = activity.date
            if isinstance(date_val, str):
                date_val = datetime.datetime.strptime(date_val, "%Y-%m-%d").date()

            idx = df.index[df['date'] == date_val].tolist()
            if idx:
                row_idx = idx[0]
                activity_type = activity.activity_type
                if activity_type == "transcription":
                    df.at[row_idx, 'transcriptions'] = activity.count
                elif activity_type == "youtube_extraction":
                    df.at[row_idx, 'youtube_extractions'] = activity.count
                elif activity_type == "video_extraction":
                    df.at[row_idx, 'video_extractions'] = activity.count
                elif activity_type == "tts_generation":
                    df.at[row_idx, 'tts_generations'] = activity.count

        return df

    except Exception as e:
        logging.error(f"Erreur lors de la r√©cup√©ration des m√©triques: {str(e)}")
        # En cas d'erreur, utiliser des donn√©es fictives
        return generate_mock_data()


def calculate_summary_metrics(df):
    """
    Calcule les m√©triques r√©sum√©es √† partir du DataFrame.
    """
    # Derniers 7 jours
    last_7_days = df.iloc[-7:]

    # Sommes des 7 derniers jours - assurez-vous que ce sont des entiers
    recent_sum = {
        'transcriptions': int(last_7_days['transcriptions'].sum()),
        'youtube_extractions': int(last_7_days['youtube_extractions'].sum()),
        'video_extractions': int(last_7_days['video_extractions'].sum()),
        'tts_generations': int(last_7_days['tts_generations'].sum())
    }

    # Log pour v√©rifier les valeurs calcul√©es
    logging.info(f"M√©triques calcul√©es: {recent_sum}")

    # Moyennes des 7 derniers jours
    recent_avg = {
        'transcriptions': round(last_7_days['transcriptions'].mean(), 1),
        'youtube_extractions': round(last_7_days['youtube_extractions'].mean(), 1),
        'video_extractions': round(last_7_days['video_extractions'].mean(), 1),
        'tts_generations': round(last_7_days['tts_generations'].mean(), 1)
    }

    # Calcul des tendances
    previous_7_days = df.iloc[-14:-7]

    trends = {}
    for metric in ['transcriptions', 'youtube_extractions', 'video_extractions', 'tts_generations']:
        recent_total = last_7_days[metric].sum()
        previous_total = previous_7_days[metric].sum()

        if previous_total > 0:
            trend_pct = ((recent_total - previous_total) / previous_total) * 100
            trends[metric] = round(trend_pct, 1)
        else:
            trends[metric] = 100.0  # Par d√©faut si pas de donn√©es ant√©rieures

    return {
        'recent_sum': recent_sum,
        'recent_avg': recent_avg,
        'trends': trends
    }

def format_trend(value):
    """
    Formate une valeur de tendance avec fl√®che et couleur.

    Args:
        value: Pourcentage de tendance

    Returns:
        Texte format√© en HTML avec fl√®che et couleur
    """
    if value > 0:
        return f"<span style='color:green'>‚ñ≤ {value}%</span>"
    elif value < 0:
        return f"<span style='color:red'>‚ñº {abs(value)}%</span>"
    else:
        return f"<span style='color:gray'>‚ñ∫ {value}%</span>"


def get_recent_activities(user_id, limit=5):
    """
    R√©cup√®re les activit√©s r√©centes de l'utilisateur.

    Args:
        user_id: ID de l'utilisateur
        limit: Nombre maximum d'activit√©s √† r√©cup√©rer

    Returns:
        Liste des activit√©s r√©centes
    """
    from core.database import UserActivity, get_db

    try:
        db = next(get_db())

        activities = db.query(UserActivity).filter(
            UserActivity.user_id == user_id
        ).order_by(
            UserActivity.created_at.desc()
        ).limit(limit).all()

        result = []
        for activity in activities:
            activity_type_mapping = {
                "transcription": "Transcription",
                "youtube_extraction": "Extraction YouTube",
                "video_extraction": "Extraction vid√©o",
                "tts_generation": "Text-to-Speech"
            }

            result.append({
                'time': activity.created_at.strftime('%H:%M'),
                'action': activity_type_mapping.get(activity.activity_type, activity.activity_type),
                'details': activity.details or "-",
                'user': st.session_state.get("username", "utilisateur")
            })

        return result

    except Exception as e:
        logging.error(f"Erreur lors de la r√©cup√©ration des activit√©s r√©centes: {e}")
        return []


def afficher_page_1():
    st.title("üéõÔ∏è Dashboard")

    # D√©tection mobile
    is_mobile = st.session_state.get("is_mobile", False)

    # V√©rifier si l'utilisateur est connect√©
    if "user_id" not in st.session_state:
        st.warning("Veuillez vous connecter pour voir votre tableau de bord.")
        return

    # Charger les donn√©es d'utilisation r√©elles
    with st.spinner("Chargement des donn√©es..."):
        usage_data = get_user_usage_metrics(st.session_state["user_id"])
        summary = calculate_summary_metrics(usage_data)

    # Carte des m√©triques principales
    st.subheader("üìä Activit√© des 7 derniers jours")

    if is_mobile:
        # Version mobile: 2 colonnes √ó 2 rang√©es
        col1, col2 = st.columns(2)
        with col1:
            st.metric(
                "Transcriptions",
                value=summary['recent_sum']['transcriptions'],
                delta=f"{summary['trends']['transcriptions']}%"
            )
        with col2:
            st.metric(
                "YouTube",
                value=summary['recent_sum']['youtube_extractions'],
                delta=f"{summary['trends']['youtube_extractions']}%"
            )

        col3, col4 = st.columns(2)
        with col3:
            st.metric(
                "Vid√©os",
                value=summary['recent_sum']['video_extractions'],
                delta=f"{summary['trends']['video_extractions']}%"
            )
        with col4:
            st.metric(
                "TTS",
                value=summary['recent_sum']['tts_generations'],
                delta=f"{summary['trends']['tts_generations']}%"
            )
    else:
        # Version desktop: 4 colonnes
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric(
                "Transcriptions",
                value=summary['recent_sum']['transcriptions'],
                delta=f"{summary['trends']['transcriptions']}%"
            )
        with col2:
            st.metric(
                "YouTube",
                value=summary['recent_sum']['youtube_extractions'],
                delta=f"{summary['trends']['youtube_extractions']}%"
            )
        with col3:
            st.metric(
                "Vid√©os",
                value=summary['recent_sum']['video_extractions'],
                delta=f"{summary['trends']['video_extractions']}%"
            )
        with col4:
            st.metric(
                "TTS",
                value=summary['recent_sum']['tts_generations'],
                delta=f"{summary['trends']['tts_generations']}%"
            )

    # Graphique d'activit√©
    st.subheader("üìà Tendances sur 30 jours")

    # Pr√©paration des donn√©es pour le graphique
    chart_data = usage_data.copy()

    # Assurez-vous que les dates sont au bon format
    try:
        # Si les dates sont des objets datetime ou date
        chart_data['formatted_date'] = chart_data['date'].apply(
            lambda x: x.strftime('%d/%m') if hasattr(x, 'strftime') else str(x)
        )
    except Exception as e:
        # Fallback en cas d'erreur
        logging.error(f"Erreur de formatage des dates: {str(e)}")
        chart_data['formatted_date'] = chart_data['date'].astype(str)

    # S√©lection des m√©triques √† afficher
    metrics_mapping = {
        'transcriptions': 'Transcriptions',
        'youtube_extractions': 'Extractions YouTube',
        'video_extractions': 'Extractions Vid√©o',
        'tts_generations': 'G√©n√©rations TTS'
    }

    # Simplifier la s√©lection sur mobile
    if is_mobile:
        chart_metrics = st.multiselect(
            "Afficher",
            options=list(metrics_mapping.keys()),
            default=['transcriptions'],
            format_func=lambda x: metrics_mapping[x]
        )
    else:
        chart_metrics = st.multiselect(
            "M√©triques √† afficher",
            options=list(metrics_mapping.keys()),
            default=['transcriptions'],
            format_func=lambda x: metrics_mapping[x]
        )

    # Si aucune m√©trique s√©lectionn√©e, afficher toutes
    if not chart_metrics:
        chart_metrics = list(metrics_mapping.keys())

    # Cr√©ation du graphique avec des donn√©es explicites
    chart_df = pd.DataFrame()
    chart_df['date'] = chart_data['formatted_date']

    for metric in chart_metrics:
        chart_df[metrics_mapping[metric]] = chart_data[metric]

    # V√©rifier que le graphique a des donn√©es non-nulles
    has_data = chart_df.iloc[:, 1:].sum().sum() > 0
    if not has_data:
        st.warning("Aucune donn√©e d'activit√© √† afficher pour cette p√©riode.")

    # G√©n√©rer le graphique
    st.line_chart(chart_df.set_index('date'))

    def get_global_statistics(user_id):
        """Calcule les statistiques globales r√©elles pour l'utilisateur"""
        from core.database import Transcription, UserActivity, get_db
        from sqlalchemy import func, and_
        import datetime

        try:
            db = next(get_db())

            # 1. Contenu total trait√© en heures (bas√© sur les transcriptions)
            transcriptions = db.query(Transcription).filter(
                Transcription.user_id == user_id
            ).all()

            total_content_minutes = sum([t.duration / 60 for t in transcriptions]) if transcriptions else 0
            total_content_hours = round(total_content_minutes / 60, 1)

            # 2. Taux de r√©ussite (activit√©s r√©ussies / total activit√©s)
            # Si pas de tracking des √©checs, on estime √† 95-99%
            success_rate = 97  # Valeur par d√©faut raisonnable

            # 3. Temps √©conomis√© estim√© (bas√© sur le contenu trait√© * facteur d'√©conomie)
            # En moyenne, la transcription manuelle prend 4-5x plus de temps
            time_saving_factor = 4.5
            saved_time = round(total_content_hours * time_saving_factor, 1)

            # 4. Pr√©cision moyenne (bas√©e sur la qualit√© du mod√®le utilis√©)
            model_accuracy = {
                "tiny": 85,
                "base": 89,
                "small": 92,
                "medium": 94,
                "large": 97
            }

            # Obtenir une distribution des mod√®les utilis√©s
            model_counts = {}
            for t in transcriptions:
                model = t.model_used
                model_counts[model] = model_counts.get(model, 0) + 1

            # Calculer la pr√©cision moyenne pond√©r√©e
            total_count = sum(model_counts.values()) if model_counts else 1
            weighted_accuracy = sum([model_counts.get(m, 0) * model_accuracy.get(m, 90)
                                     for m in model_counts]) / total_count

            accuracy = round(weighted_accuracy)

            return {
                "total_content_hours": total_content_hours,
                "success_rate": success_rate,
                "saved_time": saved_time,
                "accuracy": accuracy
            }
        except Exception as e:
            logging.error(f"Erreur lors du calcul des statistiques globales: {str(e)}")
            # Valeurs par d√©faut en cas d'erreur
            return {
                "total_content_hours": 0,
                "success_rate": 95,
                "saved_time": 0,
                "accuracy": 90
            }

    # Statistiques globales
    st.subheader("üßÆ Statistiques globales")

    # R√©cup√©rer les statistiques r√©elles
    global_stats = get_global_statistics(st.session_state["user_id"])

    if is_mobile:
        # Version mobile: empil√©
        # Temps total de contenu trait√©
        st.info(f"üíæ Contenu total trait√©: **{global_stats['total_content_hours']} heures**")

        # Taux de r√©ussite
        st.success(f"‚úÖ Taux de r√©ussite: **{global_stats['success_rate']}%**")

        # √âconomies estim√©es
        st.info(f"‚è±Ô∏è Temps √©conomis√©: **{global_stats['saved_time']} heures**")

        # Pr√©cision moyenne
        st.success(f"üéØ Pr√©cision moyenne: **{global_stats['accuracy']}%**")
    else:
        # Version desktop: colonnes
        col1, col2 = st.columns(2)
        with col1:
            # Temps total de contenu trait√©
            st.info(f"üíæ Contenu total trait√©: **{global_stats['total_content_hours']} heures**")

            # Taux de r√©ussite
            st.success(f"‚úÖ Taux de r√©ussite des op√©rations: **{global_stats['success_rate']}%**")

        with col2:
            # √âconomies estim√©es
            st.info(f"‚è±Ô∏è Temps √©conomis√© estim√©: **{global_stats['saved_time']} heures**")

            # Pr√©cision moyenne
            st.success(f"üéØ Pr√©cision moyenne: **{global_stats['accuracy']}%**")
    # Aide rapide
    with st.expander("üí° D√©marrage rapide"):
        st.markdown("""
        ### Comment utiliser TranscriptFlow:

        1. **Extraction d'audio**: Utilisez l'onglet "Extraction" pour obtenir l'audio depuis YouTube ou un fichier vid√©o local
        2. **Transcription**: Convertissez l'audio en texte avec Whisper
        3. **Analyse**: G√©n√©rez des r√©sum√©s, mots-cl√©s ou posez des questions sur le contenu
        4. **Export**: T√©l√©chargez les r√©sultats au format texte

        Besoin d'aide suppl√©mentaire? Consultez la documentation compl√®te dans les param√®tres.
        """)